draw.triple.venn(nrow(vivo_f),nrow(kenkob_f),nrow(total_f),(n.12),(n.23),(n.13),(n.123),category = c("Vivo Energy","KenolKobil","Total Kenya"),lty = "blank",fill = c("lawngreen","mediumslateblue","firebrick3"), fontfamily = rep("Century Schoolbook",7), alpha = rep(0.6,3),cat.cex = rep(1.28,3), title(main = "oil") )
grid.newpage()
draw.triple.venn(nrow(vivo_f),nrow(kenkob_f),nrow(total_f),(n.12),(n.23),(n.13),(n.123),category = c("Vivo Energy","KenolKobil","Total Kenya"),lty = "blank",fill = c("lawngreen","mediumslateblue","firebrick3"), fontfamily = rep("Century Schoolbook",7), alpha = rep(0.6,3),cat.cex = rep(1.28,3) )
rm(n.12,n.23,n.13,n.123)
knitr::opts_chunk$set(echo = F, include = F)
pack <- c("rtweet","janitor","wordcloud","RColorBrewer","tidyverse","tidytext","gridExtra","wordcloud2","lubridate","ggthemes","plotly","extrafont","VennDiagram")
pload <- function(x){
for( i in x ){
#  require returns TRUE invisibly if it was able to load package
if( ! require( i , character.only = TRUE ) ){
#  If package was not able to be loaded then install the package
install.packages( i , dependencies = TRUE )
}
#  Load package after installing
require( i , character.only = TRUE )
}
suppressPackageStartupMessages( pload(pack) )
# declutter workspace
rm(pack, pload)
vivo.tw <- read_csv("vivotweets.csv",col_names = T)
head(vivo.tw, 6)
hashtags <- vivo.tw$hashtags %>% unique()
hashtags <- drop_na(hashtags %>% as.data.frame(stringsAsFactors=F))
head(hashtags %>% as_tibble(),8)
Htags <- unnest_tokens(vivo.tw,output = tags,input = hashtags,token = "words") %>% filter(!is_retweet==TRUE) %>% select(tags) %>% drop_na() %>% count(tags, sort=T)
colnames(Htags) <- c("Hashtags","Tally")
head(Htags,8)
# checking if i can obtain a better wordcloud using wordcloud2
library(wordcloud2)
options(stringsAsFactors = F)
Htags2 <- Htags
Htags2$Tally <- log(Htags$Tally)
colnames(Htags2) <- c("word","freq")
wordcloud2(data = Htags2, size = 0.4 )
# declutter workspace
rm(Htags2)
pop <- vivo.tw %>% filter(!is_retweet==TRUE) %>% select(text,favorite_count,retweet_count)
ggplot(pop,aes(retweet_count,favorite_count)) +
geom_point(alpha=0.40) +
geom_smooth(method = "lm") +
labs(x="Retweet Count",y="Favorite Count") +
theme_solarized()
pop1 <- gather(pop,metric,tally,2:3)
ggplot(pop1 , aes(tally,colour= metric)) +
geom_freqpoly(binwidth=1 ,size=1) +
scale_colour_hue(labels = c("Favorite Count","Retweet Count") ) +
scale_x_continuous(breaks = c(seq.int(0,60,by = 10)) ) +
coord_cartesian(xlim = c(0,40)) +
labs(x=NULL,y="Count",colour="Metric",title="Distribution of Metrics") +
theme_solarized() +
theme(panel.grid.minor.x = element_line(),
legend.position = "top",
plot.title = element_text(face = "bold"))
rm(pop1)
plot1<- ggplot(pop) +
geom_histogram(aes(favorite_count),binwidth=1,fill="firebrick3",alpha=0.6,colour="slategrey") +
coord_cartesian(xlim = c(0,40)) +
labs(x=NULL,y="Count", title="Hist of Favorite Tweets") +
theme_solarized() +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
plot.title = element_text(face = "bold")
)
plot2<- ggplot(pop) +
geom_histogram(aes(retweet_count),binwidth=1,fill="mediumslateblue",alpha=0.6,colour="slategrey") +
coord_cartesian(xlim = c(0,40)) +
labs(x=NULL,y="Count", title="Hist of Retweets") +
theme_solarized() +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
plot.title = element_text(face = "bold")
)
grid.arrange(plot1,plot2, ncol=2 )
rm(plot1,plot2)
pop2 <- pop %>% arrange(desc(favorite_count),desc(retweet_count))
names(pop2) <- c("Text", "Favorite Count","Retweet Count")
head(pop2,10)
rm(pop2)
#Remove links from tweets
pop_tidy <- pop
pop_tidy$text <- gsub('http.*\\s*', '', pop_tidy$text)
pop_tidy <- pop_tidy %>% unnest_tokens(word,text) %>% anti_join(stop_words, by="word") %>% count(word,sort=TRUE)
names(pop_tidy) <- c("Word","Tally")
head(pop_tidy,10)
vivo.tw <- vivo.tw %>% mutate(wday = wday(created_at, label=T),
month = month(created_at, label=T),
year = year(created_at) )
vivo.wd <- vivo.tw %>% group_by(wday) %>% summarise(Tally = n())
vivo.wd
a <- ggplot(vivo.wd, aes(x=wday, y=Tally) ) +
geom_bar(stat = "identity", width = 0.5) +
labs(x=NULL) +
theme_solarized() +
expand_limits(y=c(0,600))
ggplotly(a)
rm(a)
# round off the dates to the nearest hour
vivo.tw$rounded_time <- vivo.tw$created_at %>% round_date("hour") %>% hour()
# frequency polygon
b<- ggplot(data=vivo.tw, aes(rounded_time) ) +
geom_bar(stat = "count") +
theme_solarized() +
scale_x_continuous(breaks=seq.int(2,20,2), labels = paste0(c(seq.int(2,12,2),c(seq.int(2,8,2))),rep(c("am","noon","pm"), c(5,1,4) ) ) ) +
labs(x=NULL, y="Count")
ggplotly(b)
# declutter workspace
rm(b)
vivo.tw %>% group_by(rounded_time) %>% summarise(median_f=median(favorite_count),median_rt=median(retweet_count))
# finding the values contained in a particular rounded hour
#vivo.tw %>% filter(rounded_time==19) %>% select(favorite_count,retweet_count)
c<- ggplot(data = vivo.tw, aes(x=rounded_time, y=favorite_count+1, group=rounded_time)) +
geom_boxplot() +
labs(x=NULL,y=NULL, title="BoxPlot of Favourite Count per Hour") +
scale_x_continuous(breaks=seq.int(2,20,2), labels = paste0(c(seq.int(2,12,2),c(seq.int(2,8,2))),rep(c("am","noon","pm"), c(5,1,4) ) ) ) +
scale_y_continuous(trans = "log") +
theme_solarized()
c
# needs extra work to be made interactive
# ggplotly(c)
# rm(c)
ggplot(data = vivo.tw, aes(x=rounded_time, y=retweet_count+1, group=rounded_time)) +
geom_boxplot() +
labs(x=NULL,y=NULL,title="BoxPlot of Retweet Count per Hour") +
scale_x_continuous(breaks=seq.int(2,20,2), labels = paste0(c(seq.int(2,12,2),c(seq.int(2,8,2))),rep(c("am","noon","pm"), c(5,1,4) ) ) ) +
coord_cartesian(ylim = c(0,60)) +
theme_solarized()#+
scale_y_continuous(trans = "log")
afinn <- get_sentiments(lexicon = "afinn")
pop_tidy.s <- vivo.tw
pop_tidy.s$text <- gsub('http.*\\s*', '', pop_tidy.s$text)
pop_tidy.s <- pop_tidy.s %>% filter(!is_retweet==TRUE) %>% select(text,33:35) %>% unnest_tokens(word,text) %>%
inner_join(afinn, by =c("word"="word"))
rm(afinn)
x <- pop_tidy.s %>% group_by(month,year) %>% summarise(total_score=sum(score))
plotx<- ggplot(filter(x, year==2016), aes(x=month, y=total_score)) +
geom_bar(stat = "identity", width = 0.5) +
labs(x=NULL, y="Total Score", title="Total Sentiment Score 2016") +
theme_solarized()
#rm(x)
# Count of tweets with sentiments in each month
y <- pop_tidy.s %>% filter(year==2016) %>% group_by(month) %>% summarise(Tally=n())
ploty<- ggplot(data = y, aes(x=month,y=Tally)) +
geom_bar(stat = "identity", width = 0.5) +
theme_solarized() +
labs(x=NULL, title="Total Tweets Per Month in 2016")
grid.arrange(plotx,ploty)
x <- pop_tidy.s %>% group_by(month,year) %>% summarise(total_score=sum(score))
d<- ggplot(filter(x, year==2017), aes(x=month, y=total_score)) +
geom_bar(stat = "identity", width = 0.6) +
labs(x=NULL, y="Total Score",title="Total Sentiment Score 2017") +
theme_solarized()
rm(x)
#ggplotly(d)
d
vivo.tw %>% group_by(source) %>% summarise(Tally=n())
vivo.tw %>% group_by(place_name) %>% summarise(Tally=n())
vivo_f <- get_followers("vivoenergykenya")
kenkob_f <- get_followers("KenolKobil")
total_f <- get_followers("totalkenya")
n.12<- semi_join(vivo_f,kenkob_f,"user_id") %>% nrow()
n.23 <- semi_join(kenkob_f,total_f,"user_id") %>% nrow()
n.13 <- semi_join(vivo_f,total_f,"user_id") %>% nrow()
n.123 <-semi_join(vivo_f,kenkob_f,"user_id") %>% semi_join(total_f,"user_id") %>% nrow()
grid.newpage()
draw.triple.venn(nrow(vivo_f),nrow(kenkob_f),nrow(total_f),(n.12),(n.23),(n.13),(n.123),category = c("Vivo Energy","KenolKobil","Total Kenya"),lty = "blank",fill = c("lawngreen","mediumslateblue","firebrick3"), fontfamily = rep("Century Schoolbook",7), alpha = rep(0.6,3),cat.cex = rep(1.28,3) )
rm(n.12,n.23,n.13,n.123)
a <- ggplot(vivo.wd, aes(x=wday, y=Tally) ) +
geom_bar(stat = "identity", width = 0.5) +
labs(x=NULL) +
theme_solarized() +
expand_limits(y=c(0,600))
#ggplotly(a)
a
rm(a)
# frequency polygon
b<- ggplot(data=vivo.tw, aes(rounded_time) ) +
geom_bar(stat = "count") +
theme_solarized() +
scale_x_continuous(breaks=seq.int(2,20,2), labels = paste0(c(seq.int(2,12,2),c(seq.int(2,8,2))),rep(c("am","noon","pm"), c(5,1,4) ) ) ) +
labs(x=NULL, y="Count")
#ggplotly(b)
b
# declutter workspace
rm(b)
knitr::opts_chunk$set(echo = F, include = F)
pack <- c("rtweet","janitor","wordcloud","RColorBrewer","tidyverse","tidytext","gridExtra","wordcloud2","lubridate","ggthemes","plotly","extrafont","VennDiagram")
pload <- function(x){
for( i in x ){
#  require returns TRUE invisibly if it was able to load package
if( ! require( i , character.only = TRUE ) ){
#  If package was not able to be loaded then install the package
install.packages( i , dependencies = TRUE )
}
#  Load package after installing
require( i , character.only = TRUE )
}
suppressPackageStartupMessages( pload(pack) )
# declutter workspace
rm(pack, pload)
vivo.tw <- read_csv("vivotweets.csv",col_names = T)
head(vivo.tw, 6)
hashtags <- vivo.tw$hashtags %>% unique()
hashtags <- drop_na(hashtags %>% as.data.frame(stringsAsFactors=F))
head(hashtags %>% as_tibble(),8)
Htags <- unnest_tokens(vivo.tw,output = tags,input = hashtags,token = "words") %>% filter(!is_retweet==TRUE) %>% select(tags) %>% drop_na() %>% count(tags, sort=T)
colnames(Htags) <- c("Hashtags","Tally")
head(Htags,8)
# checking if i can obtain a better wordcloud using wordcloud2
library(wordcloud2)
options(stringsAsFactors = F)
Htags2 <- Htags
Htags2$Tally <- log(Htags$Tally)
colnames(Htags2) <- c("word","freq")
wordcloud2(data = Htags2, size = 0.4 )
# declutter workspace
rm(Htags2)
pop <- vivo.tw %>% filter(!is_retweet==TRUE) %>% select(text,favorite_count,retweet_count)
ggplot(pop,aes(retweet_count,favorite_count)) +
geom_point(alpha=0.40) +
geom_smooth(method = "lm") +
labs(x="Retweet Count",y="Favorite Count") +
theme_solarized()
pop1 <- gather(pop,metric,tally,2:3)
ggplot(pop1 , aes(tally,colour= metric)) +
geom_freqpoly(binwidth=1 ,size=1) +
scale_colour_hue(labels = c("Favorite Count","Retweet Count") ) +
scale_x_continuous(breaks = c(seq.int(0,60,by = 10)) ) +
coord_cartesian(xlim = c(0,40)) +
labs(x=NULL,y="Count",colour="Metric",title="Distribution of Metrics") +
theme_solarized() +
theme(panel.grid.minor.x = element_line(),
legend.position = "top",
plot.title = element_text(face = "bold"))
rm(pop1)
plot1<- ggplot(pop) +
geom_histogram(aes(favorite_count),binwidth=1,fill="firebrick3",alpha=0.6,colour="slategrey") +
coord_cartesian(xlim = c(0,40)) +
labs(x=NULL,y="Count", title="Hist of Favorite Tweets") +
theme_solarized() +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
plot.title = element_text(face = "bold")
)
plot2<- ggplot(pop) +
geom_histogram(aes(retweet_count),binwidth=1,fill="mediumslateblue",alpha=0.6,colour="slategrey") +
coord_cartesian(xlim = c(0,40)) +
labs(x=NULL,y="Count", title="Hist of Retweets") +
theme_solarized() +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
plot.title = element_text(face = "bold")
)
grid.arrange(plot1,plot2, ncol=2 )
rm(plot1,plot2)
pop2 <- pop %>% arrange(desc(favorite_count),desc(retweet_count))
names(pop2) <- c("Text", "Favorite Count","Retweet Count")
head(pop2,10)
rm(pop2)
#Remove links from tweets
pop_tidy <- pop
pop_tidy$text <- gsub('http.*\\s*', '', pop_tidy$text)
pop_tidy <- pop_tidy %>% unnest_tokens(word,text) %>% anti_join(stop_words, by="word") %>% count(word,sort=TRUE)
names(pop_tidy) <- c("Word","Tally")
head(pop_tidy,10)
vivo.tw <- vivo.tw %>% mutate(wday = wday(created_at, label=T),
month = month(created_at, label=T),
year = year(created_at) )
vivo.wd <- vivo.tw %>% group_by(wday) %>% summarise(Tally = n())
vivo.wd
a <- ggplot(vivo.wd, aes(x=wday, y=Tally) ) +
geom_bar(stat = "identity", width = 0.5) +
labs(x=NULL) +
theme_solarized() +
expand_limits(y=c(0,600))
#ggplotly(a)
a
rm(a)
# round off the dates to the nearest hour
vivo.tw$rounded_time <- vivo.tw$created_at %>% round_date("hour") %>% hour()
# frequency polygon
b<- ggplot(data=vivo.tw, aes(rounded_time) ) +
geom_bar(stat = "count") +
theme_solarized() +
scale_x_continuous(breaks=seq.int(2,20,2), labels = paste0(c(seq.int(2,12,2),c(seq.int(2,8,2))),rep(c("am","noon","pm"), c(5,1,4) ) ) ) +
labs(x=NULL, y="Count")
#ggplotly(b)
b
# declutter workspace
rm(b)
vivo.tw %>% group_by(rounded_time) %>% summarise(median_f=median(favorite_count),median_rt=median(retweet_count))
# finding the values contained in a particular rounded hour
#vivo.tw %>% filter(rounded_time==19) %>% select(favorite_count,retweet_count)
c<- ggplot(data = vivo.tw, aes(x=rounded_time, y=favorite_count+1, group=rounded_time)) +
geom_boxplot() +
labs(x=NULL,y=NULL, title="BoxPlot of Favourite Count per Hour") +
scale_x_continuous(breaks=seq.int(2,20,2), labels = paste0(c(seq.int(2,12,2),c(seq.int(2,8,2))),rep(c("am","noon","pm"), c(5,1,4) ) ) ) +
scale_y_continuous(trans = "log") +
theme_solarized()
c
# needs extra work to be made interactive
# ggplotly(c)
# rm(c)
ggplot(data = vivo.tw, aes(x=rounded_time, y=retweet_count+1, group=rounded_time)) +
geom_boxplot() +
labs(x=NULL,y=NULL,title="BoxPlot of Retweet Count per Hour") +
scale_x_continuous(breaks=seq.int(2,20,2), labels = paste0(c(seq.int(2,12,2),c(seq.int(2,8,2))),rep(c("am","noon","pm"), c(5,1,4) ) ) ) +
coord_cartesian(ylim = c(0,60)) +
theme_solarized()#+
scale_y_continuous(trans = "log")
afinn <- get_sentiments(lexicon = "afinn")
pop_tidy.s <- vivo.tw
pop_tidy.s$text <- gsub('http.*\\s*', '', pop_tidy.s$text)
pop_tidy.s <- pop_tidy.s %>% filter(!is_retweet==TRUE) %>% select(text,33:35) %>% unnest_tokens(word,text) %>%
inner_join(afinn, by =c("word"="word"))
rm(afinn)
x <- pop_tidy.s %>% group_by(month,year) %>% summarise(total_score=sum(score))
plotx<- ggplot(filter(x, year==2016), aes(x=month, y=total_score)) +
geom_bar(stat = "identity", width = 0.5) +
labs(x=NULL, y="Total Score", title="Total Sentiment Score 2016") +
theme_solarized()
#rm(x)
# Count of tweets with sentiments in each month
y <- pop_tidy.s %>% filter(year==2016) %>% group_by(month) %>% summarise(Tally=n())
ploty<- ggplot(data = y, aes(x=month,y=Tally)) +
geom_bar(stat = "identity", width = 0.5) +
theme_solarized() +
labs(x=NULL, title="Total Tweets Per Month in 2016")
grid.arrange(plotx,ploty)
x <- pop_tidy.s %>% group_by(month,year) %>% summarise(total_score=sum(score))
d<- ggplot(filter(x, year==2017), aes(x=month, y=total_score)) +
geom_bar(stat = "identity", width = 0.6) +
labs(x=NULL, y="Total Score",title="Total Sentiment Score 2017") +
theme_solarized()
rm(x)
#ggplotly(d)
d
vivo.tw %>% group_by(source) %>% summarise(Tally=n())
vivo.tw %>% group_by(place_name) %>% summarise(Tally=n())
vivo_f <- get_followers("vivoenergykenya")
kenkob_f <- get_followers("KenolKobil")
total_f <- get_followers("totalkenya")
n.12<- semi_join(vivo_f,kenkob_f,"user_id") %>% nrow()
n.23 <- semi_join(kenkob_f,total_f,"user_id") %>% nrow()
n.13 <- semi_join(vivo_f,total_f,"user_id") %>% nrow()
n.123 <-semi_join(vivo_f,kenkob_f,"user_id") %>% semi_join(total_f,"user_id") %>% nrow()
grid.newpage()
draw.triple.venn(nrow(vivo_f),nrow(kenkob_f),nrow(total_f),(n.12),(n.23),(n.13),(n.123),category = c("Vivo Energy","KenolKobil","Total Kenya"),lty = "blank",fill = c("lawngreen","mediumslateblue","firebrick3"), fontfamily = rep("Century Schoolbook",7), alpha = rep(0.6,3),cat.cex = rep(1.28,3) )
rm(n.12,n.23,n.13,n.123)
knitr::opts_chunk$set(echo = F, include = F)
pack <- c("rtweet","janitor","wordcloud","RColorBrewer","tidyverse","tidytext","gridExtra","wordcloud2","lubridate","ggthemes","plotly","extrafont","VennDiagram")
pload <- function(x){
for( i in x ){
#  require returns TRUE invisibly if it was able to load package
if( ! require( i , character.only = TRUE ) ){
#  If package was not able to be loaded then install the package
install.packages( i , dependencies = TRUE )
}
#  Load package after installing
require( i , character.only = TRUE )
}
suppressPackageStartupMessages( pload(pack) )
# declutter workspace
rm(pack, pload)
vivo.tw <- read_csv("vivotweets.csv",col_names = T)
head(vivo.tw, 6)
hashtags <- vivo.tw$hashtags %>% unique()
hashtags <- drop_na(hashtags %>% as.data.frame(stringsAsFactors=F))
head(hashtags %>% as_tibble(),8)
Htags <- unnest_tokens(vivo.tw,output = tags,input = hashtags,token = "words") %>% filter(!is_retweet==TRUE) %>% select(tags) %>% drop_na() %>% count(tags, sort=T)
colnames(Htags) <- c("Hashtags","Tally")
head(Htags,8)
# checking if i can obtain a better wordcloud using wordcloud2
library(wordcloud2)
options(stringsAsFactors = F)
Htags2 <- Htags
Htags2$Tally <- log(Htags$Tally)
colnames(Htags2) <- c("word","freq")
wordcloud2(data = Htags2, size = 0.4 )
# declutter workspace
rm(Htags2)
pop <- vivo.tw %>% filter(!is_retweet==TRUE) %>% select(text,favorite_count,retweet_count)
ggplot(pop,aes(retweet_count,favorite_count)) +
geom_point(alpha=0.40) +
geom_smooth(method = "lm") +
labs(x="Retweet Count",y="Favorite Count") +
theme_solarized()
pop1 <- gather(pop,metric,tally,2:3)
ggplot(pop1 , aes(tally,colour= metric)) +
geom_freqpoly(binwidth=1 ,size=1) +
scale_colour_hue(labels = c("Favorite Count","Retweet Count") ) +
scale_x_continuous(breaks = c(seq.int(0,60,by = 10)) ) +
coord_cartesian(xlim = c(0,40)) +
labs(x=NULL,y="Count",colour="Metric",title="Distribution of Metrics") +
theme_solarized() +
theme(panel.grid.minor.x = element_line(),
legend.position = "top",
plot.title = element_text(face = "bold"))
rm(pop1)
plot1<- ggplot(pop) +
geom_histogram(aes(favorite_count),binwidth=1,fill="firebrick3",alpha=0.6,colour="slategrey") +
coord_cartesian(xlim = c(0,40)) +
labs(x=NULL,y="Count", title="Hist of Favorite Tweets") +
theme_solarized() +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
plot.title = element_text(face = "bold")
)
plot2<- ggplot(pop) +
geom_histogram(aes(retweet_count),binwidth=1,fill="mediumslateblue",alpha=0.6,colour="slategrey") +
coord_cartesian(xlim = c(0,40)) +
labs(x=NULL,y="Count", title="Hist of Retweets") +
theme_solarized() +
theme(panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
plot.title = element_text(face = "bold")
)
grid.arrange(plot1,plot2, ncol=2 )
rm(plot1,plot2)
pop2 <- pop %>% arrange(desc(favorite_count),desc(retweet_count))
names(pop2) <- c("Text", "Favorite Count","Retweet Count")
head(pop2,10)
rm(pop2)
#Remove links from tweets
pop_tidy <- pop
pop_tidy$text <- gsub('http.*\\s*', '', pop_tidy$text)
pop_tidy <- pop_tidy %>% unnest_tokens(word,text) %>% anti_join(stop_words, by="word") %>% count(word,sort=TRUE)
names(pop_tidy) <- c("Word","Tally")
head(pop_tidy,10)
vivo.tw <- vivo.tw %>% mutate(wday = wday(created_at, label=T),
month = month(created_at, label=T),
year = year(created_at) )
vivo.wd <- vivo.tw %>% group_by(wday) %>% summarise(Tally = n())
vivo.wd
a <- ggplot(vivo.wd, aes(x=wday, y=Tally) ) +
geom_bar(stat = "identity", width = 0.5) +
labs(x=NULL) +
theme_solarized() +
expand_limits(y=c(0,600))
#ggplotly(a)
a
rm(a)
# round off the dates to the nearest hour
vivo.tw$rounded_time <- vivo.tw$created_at %>% round_date("hour") %>% hour()
# frequency polygon
b<- ggplot(data=vivo.tw, aes(rounded_time) ) +
geom_bar(stat = "count") +
theme_solarized() +
scale_x_continuous(breaks=seq.int(2,20,2), labels = paste0(c(seq.int(2,12,2),c(seq.int(2,8,2))),rep(c("am","noon","pm"), c(5,1,4) ) ) ) +
labs(x=NULL, y="Count")
#ggplotly(b)
b
# declutter workspace
rm(b)
vivo.tw %>% group_by(rounded_time) %>% summarise(median_f=median(favorite_count),median_rt=median(retweet_count))
# finding the values contained in a particular rounded hour
#vivo.tw %>% filter(rounded_time==19) %>% select(favorite_count,retweet_count)
c<- ggplot(data = vivo.tw, aes(x=rounded_time, y=favorite_count+1, group=rounded_time)) +
geom_boxplot() +
labs(x=NULL,y=NULL, title="BoxPlot of Favourite Count per Hour") +
scale_x_continuous(breaks=seq.int(2,20,2), labels = paste0(c(seq.int(2,12,2),c(seq.int(2,8,2))),rep(c("am","noon","pm"), c(5,1,4) ) ) ) +
scale_y_continuous(trans = "log") +
theme_solarized()
c
# needs extra work to be made interactive
# ggplotly(c)
# rm(c)
ggplot(data = vivo.tw, aes(x=rounded_time, y=retweet_count+1, group=rounded_time)) +
geom_boxplot() +
labs(x=NULL,y=NULL,title="BoxPlot of Retweet Count per Hour") +
scale_x_continuous(breaks=seq.int(2,20,2), labels = paste0(c(seq.int(2,12,2),c(seq.int(2,8,2))),rep(c("am","noon","pm"), c(5,1,4) ) ) ) +
coord_cartesian(ylim = c(0,60)) +
theme_solarized()#+
scale_y_continuous(trans = "log")
afinn <- get_sentiments(lexicon = "afinn")
pop_tidy.s <- vivo.tw
pop_tidy.s$text <- gsub('http.*\\s*', '', pop_tidy.s$text)
pop_tidy.s <- pop_tidy.s %>% filter(!is_retweet==TRUE) %>% select(text,33:35) %>% unnest_tokens(word,text) %>%
inner_join(afinn, by =c("word"="word"))
rm(afinn)
x <- pop_tidy.s %>% group_by(month,year) %>% summarise(total_score=sum(score))
plotx<- ggplot(filter(x, year==2016), aes(x=month, y=total_score)) +
geom_bar(stat = "identity", width = 0.5) +
labs(x=NULL, y="Total Score", title="Total Sentiment Score 2016") +
theme_solarized()
#rm(x)
# Count of tweets with sentiments in each month
y <- pop_tidy.s %>% filter(year==2016) %>% group_by(month) %>% summarise(Tally=n())
ploty<- ggplot(data = y, aes(x=month,y=Tally)) +
geom_bar(stat = "identity", width = 0.5) +
theme_solarized() +
labs(x=NULL, title="Total Tweets Per Month in 2016")
grid.arrange(plotx,ploty)
x <- pop_tidy.s %>% group_by(month,year) %>% summarise(total_score=sum(score))
d<- ggplot(filter(x, year==2017), aes(x=month, y=total_score)) +
geom_bar(stat = "identity", width = 0.6) +
labs(x=NULL, y="Total Score",title="Total Sentiment Score 2017") +
theme_solarized()
rm(x)
#ggplotly(d)
d
vivo.tw %>% group_by(source) %>% summarise(Tally=n())
vivo.tw %>% group_by(place_name) %>% summarise(Tally=n())
vivo_f <- get_followers("vivoenergykenya")
kenkob_f <- get_followers("KenolKobil")
total_f <- get_followers("totalkenya")
n.12<- semi_join(vivo_f,kenkob_f,"user_id") %>% nrow()
n.23 <- semi_join(kenkob_f,total_f,"user_id") %>% nrow()
n.13 <- semi_join(vivo_f,total_f,"user_id") %>% nrow()
n.123 <-semi_join(vivo_f,kenkob_f,"user_id") %>% semi_join(total_f,"user_id") %>% nrow()
grid.newpage()
draw.triple.venn(nrow(vivo_f),nrow(kenkob_f),nrow(total_f),(n.12),(n.23),(n.13),(n.123),category = c("Vivo Energy","KenolKobil","Total Kenya"),lty = "blank",fill = c("lawngreen","mediumslateblue","firebrick3"), fontfamily = rep("Century Schoolbook",7), alpha = rep(0.6,3),cat.cex = rep(1.28,3) )
rm(n.12,n.23,n.13,n.123)
